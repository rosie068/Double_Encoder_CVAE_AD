{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf60065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_slices(slices):\n",
    "    #Function to display row of image slices\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "def print_img(img):\n",
    "    slice_0 = img[img.shape[0]//2,:,:]\n",
    "    slice_1 = img[:,img.shape[1]//2, :]\n",
    "    slice_2 = img[:,:,img.shape[2]//2]\n",
    "    show_slices([slice_0, slice_1, slice_2])\n",
    "\n",
    "def print_coronal(img, title):\n",
    "    slice_1 = img[:,img.shape[1]//2, :]\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    # Plot the image on the axes\n",
    "    ax.imshow(slice_1.T, cmap=\"gray\", origin=\"lower\")\n",
    "    # Show the plot\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c05cf",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ecafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"patched_paired_input_CVAE.csv\", index_col=0)\n",
    "print(df['age1'].min(), df['age1'].max(), df['timeDiff'].min(), df['timeDiff'].max())\n",
    "print(df['status'].value_counts())\n",
    "\n",
    "## will do this later with the images\n",
    "df['age1'] = (df['age1']-df['age1'].mean())/df['age1'].std()\n",
    "print(df['age1'].mean(), df['age1'].std())\n",
    "\n",
    "timeDiff_mean = df['timeDiff'].mean()\n",
    "timeDiff_std = df['timeDiff'].std()\n",
    "\n",
    "df['timeDiff'] = (df['timeDiff']-df['timeDiff'].mean())/df['timeDiff'].std() \n",
    "print(df['timeDiff'].mean(),df['timeDiff'].std())\n",
    "\n",
    "# df\n",
    "\n",
    "pair_imgs = df.values\n",
    "pair_imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcac4c8",
   "metadata": {},
   "source": [
    "# VAE model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf44475",
   "metadata": {},
   "source": [
    "### dataloader, each input is [img2, img1], and scalar input [age1, timediff, status, one hot encoding for patch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "# build data loader\n",
    "class BrainDataset(Dataset):\n",
    "    \n",
    "    '''Load a random 3D image'''\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "        self.n_files = len(self.files) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        \n",
    "        path = \"\"\n",
    "        image1 = torch.Tensor(np.load(path+fname[0])).unsqueeze(0)\n",
    "        image2 = torch.Tensor(np.load(path+fname[2])).unsqueeze(0)\n",
    "        \n",
    "        age1 = fname[1]\n",
    "        age2 = fname[-2]\n",
    "        status = fname[4]\n",
    "        patch_num = fname[-1]\n",
    "        \n",
    "        ## we want multi-channel images, so size stays the same\n",
    "        ## input shape [batch_size, 2, 45,45,45]\n",
    "        paired_inputs = torch.cat((image2, image1), 0)\n",
    "        scalar_vars = [age1, age2, status, 0,0,0,0,0,0,0,0]\n",
    "        scalar_vars[patch_num+3] = 1\n",
    "\n",
    "        return paired_inputs.to(torch.float32), torch.Tensor(scalar_vars).to(torch.float32)\n",
    "            \n",
    "b_size = 32\n",
    "\n",
    "dataset = BrainDataset(pair_imgs)\n",
    "dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c29887",
   "metadata": {},
   "source": [
    "## basic blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04093e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for reproducibility\n",
    "# torch.manual_seed(1999)\n",
    "\n",
    "# misc\n",
    "def parameter_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# helper block function\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scalar_size=11):\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=True)\n",
    "        self.linear = nn.Linear(scalar_size, out_channels)\n",
    "        self.norm = nn.GroupNorm(4, out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, sca):\n",
    "        x = self.conv(x)\n",
    "        sca = self.linear(sca)\n",
    "        \n",
    "        x = x + sca.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.norm(x)\n",
    "        return self.relu(x)\n",
    "    \n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, pad, out_pad=[0,0,0]):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.convTran = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding=pad,\n",
    "                               output_padding=out_pad, bias=False),\n",
    "            nn.GroupNorm(4, out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convTran(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfe370",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e8629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        base = 64\n",
    "        kernel_size = [3,3,3]\n",
    "        stride_size = [1,1,1]\n",
    "        padding_size = [1,1,1]\n",
    "\n",
    "        maxpool_kernel_size = [2,2,2]\n",
    "        maxpool_stride_size = [2,2,2]\n",
    "        \n",
    "        convTrans_kernel = [2,2,2]\n",
    "        convTrans_stride = [2,2,2]\n",
    "        \n",
    "        ### ENCODER: 4 layers of convolutions + 3 downsampling\n",
    "        self.e1 = Conv(2, base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e2 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p1 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e3 = Conv(base, 2*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p2 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e5 = Conv(2*base, 4*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e6 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p3 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e7 = Conv(4*base, 8*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e8 = Conv(8*base, 8*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.last_linear = nn.Linear(5*5*5*8*base, 20) #4*4*4*8*base, 20)\n",
    "        \n",
    "        \n",
    "        ## U-Net decoder, encoder part for only the image+conditionals\n",
    "        self.u_down1 = Conv(1, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_down2 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_pool1 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.u_down3 = Conv(base, 2*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_down4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_pool2 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.u_down5 = Conv(2*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_down6 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_pool3 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        ## decoder latent space add directly here at the bottleneck\n",
    "        self.u_down7 = Conv(4*base, 8*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.u_down8 = Conv(8*base, 8*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        \n",
    "        ## decoder up part\n",
    "        self.t1 = ConvTranspose(8*base, 4*base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0], out_pad=[1,1,1])\n",
    "        self.d1 = Conv(8*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.d2 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "\n",
    "        self.t2 = ConvTranspose(4*base, 2*base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0])\n",
    "        self.d3 = Conv(4*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.d4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "\n",
    "        self.t3 = ConvTranspose(2*base, base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0], out_pad=[1,1,1])\n",
    "        self.d5 = Conv(2*base, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        self.d6 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=21)\n",
    "        \n",
    "        self.last_conv = nn.Conv3d(base, 1, [1,1,1], [1,1,1], bias=True)\n",
    "\n",
    "    def encode(self, x, scalar):\n",
    "        x = self.e1(x, scalar)\n",
    "        x = self.e2(x, scalar)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.e3(x, scalar)\n",
    "        x = self.e4(x, scalar)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.e5(x, scalar)\n",
    "        x = self.e6(x, scalar)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.e7(x, scalar)\n",
    "        x = self.e8(x, scalar)\n",
    "        return self.last_linear(x.view(x.size(0), -1))\n",
    "    \n",
    "    \n",
    "    def to_mu_sigma(self, out):\n",
    "        mu = out[:,:10]\n",
    "        logsigma2 = out[:,10:]  \n",
    "\n",
    "        sigma = torch.exp(0.5*logsigma2)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        tmp1 = torch.randn(mu.shape).to(device)\n",
    "        return mu + tmp1*sigma\n",
    "    \n",
    "    def decode(self, z, scalar, img1):\n",
    "        latent_scalar = torch.cat((z, scalar), dim=1)\n",
    "        \n",
    "        ## u-net strucutre, downsize part just img 1\n",
    "        out = self.u_down1(img1, latent_scalar)\n",
    "        skip1 = self.u_down2(out, latent_scalar)\n",
    "        out = self.u_pool1(skip1)\n",
    "        \n",
    "        out = self.u_down3(out, latent_scalar)\n",
    "        skip2 = self.u_down4(out, latent_scalar)\n",
    "        out = self.u_pool2(skip2)\n",
    "        \n",
    "        out = self.u_down5(out, latent_scalar)\n",
    "        skip3 = self.u_down6(out, latent_scalar)\n",
    "        out = self.u_pool3(skip3)\n",
    "        \n",
    "        out = self.u_down7(out, latent_scalar)\n",
    "        out = self.u_down8(out, latent_scalar)\n",
    "        \n",
    "        ## u-net strucutre, upsize part\n",
    "        out = self.t1(out)\n",
    "        out = self.d1(torch.cat([out, skip3], 1), latent_scalar)\n",
    "        out = self.d2(out, latent_scalar)\n",
    "        \n",
    "        out = self.t2(out)\n",
    "        out = self.d3(torch.cat([out, skip2], 1), latent_scalar)\n",
    "        out = self.d4(out, latent_scalar)\n",
    "        \n",
    "        out = self.t3(out)\n",
    "        out = self.d5(torch.cat([out, skip1], 1), latent_scalar)\n",
    "        out = self.d6(out, latent_scalar)\n",
    "                \n",
    "        return self.last_conv(out)\n",
    "\n",
    "    \n",
    "    def forward(self, x, scalar, img1):\n",
    "        ## out is size (batch, 20)\n",
    "        out = self.encode(x, scalar)\n",
    "                \n",
    "        mu, sigma = self.to_mu_sigma(out)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        latent_scalar = torch.cat((z, scalar), dim=1)\n",
    "        \n",
    "        ## u-net strucutre, downsize part just img 1\n",
    "        out = self.u_down1(img1, latent_scalar)\n",
    "        skip1 = self.u_down2(out, latent_scalar)\n",
    "        out = self.u_pool1(skip1)\n",
    "        \n",
    "        out = self.u_down3(out, latent_scalar)\n",
    "        skip2 = self.u_down4(out, latent_scalar)\n",
    "        out = self.u_pool2(skip2)\n",
    "        \n",
    "        out = self.u_down5(out, latent_scalar)\n",
    "        skip3 = self.u_down6(out, latent_scalar)\n",
    "        out = self.u_pool3(skip3)\n",
    "        \n",
    "        out = self.u_down7(out, latent_scalar)\n",
    "        out = self.u_down8(out, latent_scalar)\n",
    "        \n",
    "        ## u-net strucutre, upsize part\n",
    "        out = self.t1(out)\n",
    "        out = self.d1(torch.cat([out, skip3], 1), latent_scalar)\n",
    "        out = self.d2(out, latent_scalar)\n",
    "        \n",
    "        out = self.t2(out)\n",
    "        out = self.d3(torch.cat([out, skip2], 1), latent_scalar)\n",
    "        out = self.d4(out, latent_scalar)\n",
    "        \n",
    "        out = self.t3(out)\n",
    "        out = self.d5(torch.cat([out, skip1], 1), latent_scalar)\n",
    "        out = self.d6(out, latent_scalar)\n",
    "        \n",
    "        out = self.last_conv(out)\n",
    "        \n",
    "        return out, mu, sigma\n",
    "\n",
    "print('parameter count:', parameter_count(CVAE()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5076e8d",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-5\n",
    "num_epochs = 1000\n",
    "# sigma = 0.1/Istd \n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# model\n",
    "cvae = CVAE()\n",
    "cvae.to(device)\n",
    "cvae.cuda()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=init_lr)\n",
    "\n",
    "# criterion, might need to add term for how accurate/noisy the samples are, include the sigma\n",
    "# since I standardized my data, my standard deviation is 0.1. Set it to be small so the KLD is\n",
    "# negligible, we just have reconstruction\n",
    "l2_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "def kld_loss(mu,sigma2):\n",
    "    return 0.5*torch.sum(mu**2 + sigma2 - 1 - torch.log(sigma2))\n",
    "\n",
    "## if image has lots of variance, the generator just outputs an average images\n",
    "## if has small amount of variance, it seems to fit the samples well but difficult to interpolate\n",
    "def criterion(img2_batch, recon, mu, sigma2):\n",
    "    recon_loss = l2_loss(recon, img2_batch)/(2*0.1*0.1)\n",
    "    kl_loss = kld_loss(mu,sigma2)\n",
    "    return recon_loss+kl_loss, recon_loss.item(), kl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f8b30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# statistics\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "recon_loss_arr = []\n",
    "kl_loss_arr = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cvae.train(True)\n",
    "    running_loss = 0.0\n",
    "    recon_loss_e = 0.0\n",
    "    kl_loss_e = 0.0\n",
    "\n",
    "    for img_batch, scalar_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        img_batch = img_batch.cuda()\n",
    "        scalar_batch = scalar_batch.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        recon, mu, sigma = cvae(img_batch, scalar_batch, img_batch[:,1:2,...])\n",
    "        loss, recon_l, kl_l = criterion(img_batch[:,0:1,...], recon, mu, sigma**2)\n",
    "       \n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        recon_loss_e += recon_l\n",
    "        kl_loss_e += kl_l\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "    running_loss /= (pair_imgs.shape[0] // b_size)\n",
    "    recon_loss_e /= (pair_imgs.shape[0] // b_size)\n",
    "    kl_loss_e /= (pair_imgs.shape[0] // b_size)\n",
    "    \n",
    "    train_losses.append(running_loss)\n",
    "    recon_loss_arr.append(recon_loss_e)\n",
    "    kl_loss_arr.append(kl_loss_e)\n",
    "    \n",
    "    # output\n",
    "    print('Epoch {} -- loss: {:.4f}'.format(epoch + 1, running_loss))\n",
    "    \n",
    "    if epoch%100 == 99:\n",
    "        torch.save(cvae, 'CVAE_'+str(epoch)+'.pt')\n",
    "        np.savetxt(\"CVAE_recon\", recon_loss_arr)\n",
    "        np.savetxt(\"CVAE_kl\", kl_loss_arr)\n",
    "\n",
    "    # visualize reconstruction and synthesis\n",
    "    if epoch==num_epochs-1:\n",
    "        torch.save(cvae.state_dict(), 'CVAE.state_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60e4e9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recon_loss, linestyle = 'dotted', label='reconstruction loss')\n",
    "plt.plot(kl_loss, linestyle = 'dotted', label='KL loss')\n",
    "# plt.ylim(0, 50)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a42c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
