{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf60065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import imageio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be14995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization functions\n",
    "def show_slices(slices):\n",
    "    #Function to display row of image slices\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "def print_img(img):\n",
    "    slice_0 = img[img.shape[0]//2,:,:]\n",
    "    slice_1 = img[:,img.shape[1]//2, :]\n",
    "    slice_2 = img[:,:,img.shape[2]//2]\n",
    "    show_slices([slice_0, slice_1, slice_2])\n",
    "    plt.show()\n",
    "\n",
    "def print_coronal(img, title=\"\"):\n",
    "    slice_1 = img[:,img.shape[1]//2, :]\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    # Plot the image on the axes\n",
    "    ax.imshow(slice_1.T, cmap=\"gray\", origin=\"lower\")\n",
    "    # Show the plot\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c05cf",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0271c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"samples.csv\", index_col=0)\n",
    "\n",
    "## standardize age and timeDiff\n",
    "age_mean = df['age1'].mean()\n",
    "age_std = df['age1'].std()\n",
    "\n",
    "df['age1'] = (df['age1']-df['age1'].mean())/df['age1'].std()\n",
    "print(df['age1'].mean(), df['age1'].std())\n",
    "\n",
    "timeDiff_mean = df['timeDiff'].mean()\n",
    "timeDiff_std = df['timeDiff'].std()\n",
    "\n",
    "df['timeDiff'] = (df['timeDiff']-df['timeDiff'].mean())/df['timeDiff'].std() \n",
    "print(df['timeDiff'].mean(),df['timeDiff'].std())\n",
    "\n",
    "pair_imgs = df.values\n",
    "pair_imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf44475",
   "metadata": {},
   "source": [
    "# VAE model\n",
    "### dataloader, each input is [img2, img1], and scalar input [age1, timeDiff, status]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479078a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader\n",
    "img_path = \"cropped_imgs\"\n",
    "\n",
    "class BrainDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "        self.n_files = len(self.files) \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_files\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        fname = self.files[idx]\n",
    "        \n",
    "        image1 = torch.Tensor(np.load(img_path+fname[0]))\n",
    "        image2 = torch.Tensor(np.load(img_path+fname[2]))\n",
    "                \n",
    "        age1 = fname[1]\n",
    "        timediff = fname[4]\n",
    "        status = fname[3]\n",
    "        \n",
    "        ## we want multi-channel images, so size stays the same\n",
    "        ## input shape [batch_size, 2, 80,80,80]\n",
    "        paired_inputs = torch.cat((image2, image1), 0).squeeze()\n",
    "        scalar_vars = [age1, timediff, status]\n",
    "        return paired_inputs.to(torch.float32), torch.Tensor(scalar_vars).to(torch.float32)\n",
    "            \n",
    "b_size = 32\n",
    "\n",
    "dataset = BrainDataset(pair_imgs)\n",
    "dataloader = DataLoader(dataset, batch_size=b_size, shuffle=True, num_workers=8)\n",
    "\n",
    "# test_dataset = BrainDataset(test_pair_imgs)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=b_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00351143",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04093e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "def parameter_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# helper block function\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scalar_size=3):\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding, bias=True)\n",
    "        self.linear = nn.Linear(scalar_size, out_channels)\n",
    "        self.norm = nn.GroupNorm(4, out_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, sca):\n",
    "        x = self.conv(x)\n",
    "        sca = self.linear(sca)\n",
    "        \n",
    "        x = x + sca.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.norm(x)\n",
    "        return self.relu(x)\n",
    "    \n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, pad, out_pad=[0,0,0]):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        \n",
    "        self.convTran = nn.Sequential(\n",
    "            nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride, padding=pad,\n",
    "                               output_padding=out_pad, bias=False),\n",
    "            nn.GroupNorm(4, out_channels),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.convTran(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56e8629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count: 138905597\n"
     ]
    }
   ],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        base = 64\n",
    "        kernel_size = [3,3,3]\n",
    "        stride_size = [1,1,1]\n",
    "        padding_size = [1,1,1]\n",
    "\n",
    "        maxpool_kernel_size = [2,2,2]\n",
    "        maxpool_stride_size = [2,2,2]\n",
    "        \n",
    "        convTrans_kernel = [2,2,2]\n",
    "        convTrans_stride = [2,2,2]\n",
    "        \n",
    "        ### ENCODER: 4 blocks of convolutions + 3 downsampling\n",
    "        self.e1 = Conv(2, base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e2 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p1 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e3 = Conv(base, 2*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p2 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e5 = Conv(2*base, 4*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e6 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.p3 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.e7 = Conv(4*base, 8*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        self.e8 = Conv(8*base, 8*base, kernel_size, stride=stride_size, padding=padding_size)\n",
    "        \n",
    "        self.first_linear = nn.Linear(10*10*10*8*base, 200)\n",
    "        self.last_linear = nn.Linear(200, 20)\n",
    "        \n",
    "        ## U-Net decoder, encoder part for only the image+conditionals\n",
    "        self.u_down1 = Conv(1, base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_down2 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_pool1 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.u_down3 = Conv(base, 2*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_down4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_pool2 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        self.u_down5 = Conv(2*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_down6 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_pool3 = nn.MaxPool3d(kernel_size=maxpool_kernel_size, stride=maxpool_stride_size)\n",
    "\n",
    "        ## decoder latent space add directly here at the bottleneck\n",
    "        self.u_down7 = Conv(4*base, 8*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        self.u_down8 = Conv(8*base, 8*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                            scalar_size=13)\n",
    "        \n",
    "        ## decoder up part\n",
    "        self.t1 = ConvTranspose(8*base, 4*base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0])\n",
    "        self.d1 = Conv(8*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                       scalar_size=13)\n",
    "        self.d2 = Conv(4*base, 4*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                       scalar_size=13)\n",
    "\n",
    "        self.t2 = ConvTranspose(4*base, 2*base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0])\n",
    "        self.d3 = Conv(4*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                       scalar_size=13)\n",
    "        self.d4 = Conv(2*base, 2*base, kernel_size, stride=stride_size, padding=padding_size, \n",
    "                       scalar_size=13)\n",
    "\n",
    "        self.t3 = ConvTranspose(2*base, base, convTrans_kernel, stride=convTrans_stride, \n",
    "                                pad=[0,0,0])\n",
    "        self.d5 = Conv(2*base, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=13)\n",
    "        self.d6 = Conv(base, base, kernel_size, stride=stride_size, padding=padding_size, scalar_size=13)\n",
    "        \n",
    "        self.last_conv = nn.Conv3d(base, 1, [1,1,1], [1,1,1], bias=True)\n",
    "\n",
    "    def encode(self, x, scalar):\n",
    "        x = self.e1(x, scalar)\n",
    "        x = self.e2(x, scalar)\n",
    "        x = self.p1(x)\n",
    "        \n",
    "        x = self.e3(x, scalar)\n",
    "        x = self.e4(x, scalar)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = self.e5(x, scalar)\n",
    "        x = self.e6(x, scalar)\n",
    "        x = self.p3(x)\n",
    "        \n",
    "        x = self.e7(x, scalar)\n",
    "        x = self.e8(x, scalar)\n",
    "        \n",
    "        x = self.first_linear(x.view(x.size(0), -1))\n",
    "        return self.last_linear(x)\n",
    "    \n",
    "    \n",
    "    def to_mu_sigma(self, out):\n",
    "        mu = out[:,:10]\n",
    "        logsigma2 = out[:,10:]  \n",
    "\n",
    "        sigma = torch.exp(0.5*logsigma2)\n",
    "        return mu, sigma\n",
    "    \n",
    "    def reparameterize(self, mu, sigma):\n",
    "        tmp1 = torch.randn(mu.shape).to(device)\n",
    "        return mu + tmp1*sigma\n",
    "    \n",
    "    def decode(self, z, scalar, img1):\n",
    "        latent_scalar = torch.cat((z, scalar), dim=1)\n",
    "        \n",
    "        ## u-net strucutre, downsize part just img 1\n",
    "        out = self.u_down1(img1, latent_scalar)\n",
    "        skip1 = self.u_down2(out, latent_scalar)\n",
    "        out = self.u_pool1(skip1)\n",
    "        \n",
    "        out = self.u_down3(out, latent_scalar)\n",
    "        skip2 = self.u_down4(out, latent_scalar)\n",
    "        out = self.u_pool2(skip2)\n",
    "        \n",
    "        out = self.u_down5(out, latent_scalar)\n",
    "        skip3 = self.u_down6(out, latent_scalar)\n",
    "        out = self.u_pool3(skip3)\n",
    "        \n",
    "        out = self.u_down7(out, latent_scalar)\n",
    "        out = self.u_down8(out, latent_scalar)\n",
    "        \n",
    "        ## u-net strucutre, upsize part\n",
    "        out = self.t1(out)\n",
    "        out = self.d1(torch.cat([out, skip3], 1), latent_scalar)\n",
    "        out = self.d2(out, latent_scalar)\n",
    "        \n",
    "        out = self.t2(out)\n",
    "        out = self.d3(torch.cat([out, skip2], 1), latent_scalar)\n",
    "        out = self.d4(out, latent_scalar)\n",
    "        \n",
    "        out = self.t3(out)\n",
    "        out = self.d5(torch.cat([out, skip1], 1), latent_scalar)\n",
    "        out = self.d6(out, latent_scalar)\n",
    "                \n",
    "        return self.last_conv(out)\n",
    "\n",
    "    \n",
    "    def forward(self, x, scalar, img1):\n",
    "        ## out is size (batch, 20)\n",
    "        out = self.encode(x, scalar)\n",
    "                \n",
    "        mu, sigma = self.to_mu_sigma(out)\n",
    "        z = self.reparameterize(mu, sigma)\n",
    "        \n",
    "        out = self.decode(z, scalar, img1)\n",
    "        return out, mu, sigma\n",
    "\n",
    "print('parameter count:', parameter_count(CVAE()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5076e8d",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2514e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "init_lr = 1e-5\n",
    "num_epochs = 1000\n",
    "\n",
    "# for early stopping\n",
    "min_loss = 999999999\n",
    "patience = 10\n",
    "pat_count = 0\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# model\n",
    "cvae = CVAE()\n",
    "cvae.to(device)\n",
    "cvae.cuda()\n",
    "\n",
    "# optimizer and loss function\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=init_lr) #, weight_decay=0.001)\n",
    "l2_loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "def kld_loss(mu,sigma2):\n",
    "    return 0.5*torch.mean(mu**2 + sigma2 - 1 - torch.log(sigma2))\n",
    "\n",
    "def criterion(img2_batch, recon, mu, sigma2): #, kl_w):\n",
    "    recon_loss = l2_loss(recon, img2_batch)/(2*0.1*0.1)\n",
    "    kl_loss = kld_loss(mu,sigma2)\n",
    "    return recon_loss+kl_loss, recon_loss.item(), kl_loss.item()\n",
    "    \n",
    "# statistics\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "recon_loss_arr = []\n",
    "kl_loss_arr = []\n",
    "\n",
    "test_recon_loss_arr = []\n",
    "test_kl_loss_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    cvae.train(True)\n",
    "    running_loss = 0.0\n",
    "    recon_loss = 0.0\n",
    "    kl_loss = 0.0\n",
    "    \n",
    "    for img_batch, scalar_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        img_batch = img_batch.cuda()\n",
    "        scalar_batch = scalar_batch.cuda()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        recon, mu, sigma = cvae(img_batch, scalar_batch, img_batch[:,1:2,...])\n",
    "        loss, recon_l, kl_l = criterion(img_batch[:,0:1,...], recon, mu, sigma**2) #, kl_w)\n",
    "       \n",
    "        # statistics\n",
    "        running_loss += loss.item()\n",
    "        recon_loss += recon_l\n",
    "        kl_loss += kl_l\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "    running_loss /= (pair_imgs.shape[0] // b_size)\n",
    "    recon_loss /= (pair_imgs.shape[0] // b_size)\n",
    "    kl_loss /= (pair_imgs.shape[0] // b_size)\n",
    "    \n",
    "    train_losses.append(running_loss)\n",
    "    recon_loss_arr.append(recon_loss)\n",
    "    kl_loss_arr.append(kl_loss)\n",
    "    \n",
    "    ### early stopping\n",
    "    if running_loss < min_loss:\n",
    "        min_loss = running_loss\n",
    "        pat_count = 0\n",
    "        \n",
    "        torch.save(cvae.state_dict(), 'CVAE.state_dict')\n",
    "        \n",
    "        np.savetxt(\"CVAE_recon\", recon_loss_arr)\n",
    "        np.savetxt(\"CVAE_kl\", kl_loss_arr)\n",
    "        np.savetxt(\"CVAE_recon_test\", test_recon_loss_arr)\n",
    "        np.savetxt(\"CVAE_kl_test\", test_kl_loss_arr)\n",
    "    else:\n",
    "        if pat_count >= patience:\n",
    "            break\n",
    "        else:\n",
    "            pat_count += 1\n",
    "    \n",
    "    ### evaluate\n",
    "    cvae.eval()\n",
    "    \n",
    "    test_running_loss = 0.0\n",
    "    test_recon_loss = 0.0\n",
    "    test_kl_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_batch, scalar_batch in test_dataloader:\n",
    "            img_batch = img_batch.cuda()\n",
    "            scalar_batch = scalar_batch.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            recon, mu, sigma = cvae(img_batch, scalar_batch, img_batch[:,1:2,...])\n",
    "            test_loss, test_recon_l, test_kl_l = criterion(img_batch[:,0:1,...], recon, mu, sigma**2)\n",
    "\n",
    "            # statistics\n",
    "            test_running_loss += test_loss.item()\n",
    "            test_recon_loss += test_recon_l\n",
    "            test_kl_loss += test_kl_l\n",
    "                \n",
    "    test_running_loss /= (test_pair_imgs.shape[0] // b_size)\n",
    "    test_recon_loss /= (test_pair_imgs.shape[0] // b_size)\n",
    "    test_kl_loss /= (test_pair_imgs.shape[0] // b_size)\n",
    "    \n",
    "    test_losses.append(test_running_loss)\n",
    "    test_recon_loss_arr.append(test_recon_loss)\n",
    "    test_kl_loss_arr.append(test_kl_loss)\n",
    "\n",
    "    # output\n",
    "    print('Epoch {} -- training loss: {:.4f}, testing loss: {:.4f}'.format(epoch + 1, running_loss, \n",
    "                                                                           test_running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60e4e9",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcb8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recon_loss_arr, linestyle = 'dotted', label='reconstruction loss')\n",
    "plt.plot(kl_loss_arr, linestyle = 'dotted', label='KL loss')\n",
    "plt.plot(test_recon_loss_arr, linestyle = 'dotted', label='test recon loss')\n",
    "plt.plot(test_kl_loss_arr, linestyle = 'dotted', label='test KL loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a657f",
   "metadata": {},
   "source": [
    "### predict 10 year trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ef8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.eval()\n",
    "\n",
    "## sample z from latent space\n",
    "random_mu = torch.randn(10)\n",
    "random_var = torch.ones(1,10)\n",
    "random_mu = random_mu.cuda()\n",
    "random_var = random_var.cuda()\n",
    "\n",
    "z = cvae.reparameterize(random_mu, random_var)\n",
    "\n",
    "base_image = \"\"\n",
    "base_age = 0\n",
    "status = 5\n",
    "\n",
    "## for visualization, only take coronal middle slice\n",
    "slices = np.zeros((11,80,80))\n",
    "\n",
    "for a in range(11):\n",
    "    time_diff = (a-timeDiff_mean)/timeDiff_std\n",
    "    \n",
    "    scalar_vars = torch.Tensor([base_age, time_diff, status]).reshape((1,3)).cuda()\n",
    "\n",
    "    img1 = torch.Tensor(np.load(\"cropped_imgs/\"+base_image).squeeze())\n",
    "    img1_in = torch.Tensor(img1).unsqueeze(dim=0).to(torch.float32).cuda()\n",
    "    \n",
    "    rand_gen = cvae.decode(z, scalar_vars, img1_in).data.cpu().numpy().squeeze()\n",
    "\n",
    "    np.save(\"output_images/ADNI_year_\"+str(a)+\"_status_\"+str(status)+\".npy\", rand_gen)\n",
    "    \n",
    "    slice_1 = rand_gen[:,rand_gen.shape[1]//2, :]\n",
    "    slices[a,:] = slice_1\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    # Plot the image on the axes\n",
    "    ax.imshow(slice_1.T, cmap=\"gray\", origin=\"lower\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.suptitle(\"predicted at time difference\"+str(a))\n",
    "    plt.savefig(\"output_images/1_ADNI_nopatch_\"+str(a)+\"_status_\"+str(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c114f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "738b9a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_image = \"012_S_4094_MPRAGE_2011-07-07_13_21_27.0.npy\"\n",
    "base_age = -1.763316 #(55.1-ADNI_age_mean)/ADNI_age_std\n",
    "status = 4\n",
    "\n",
    "slices = np.zeros((11,80,80))\n",
    "output_path = \"\"\n",
    "\n",
    "for a in range(11):\n",
    "    time_diff = (a-timeDiff_mean)/timeDiff_std\n",
    "    \n",
    "    scalar_vars = torch.Tensor([base_age, time_diff, status]).reshape((1,3)).cuda()\n",
    "\n",
    "    img1 = torch.Tensor(np.load(img_path+base_image).squeeze())\n",
    "    img1_in = torch.Tensor(img1).unsqueeze(dim=0).to(torch.float32).cuda()\n",
    "    \n",
    "    rand_gen = cvae.decode(z, scalar_vars, img1_in).data.cpu().numpy().squeeze()\n",
    "\n",
    "    np.save(output_path+\"year_\"+str(a)+\"_status_\"+str(status)+\".npy\", rand_gen)\n",
    "    \n",
    "    slice_1 = rand_gen[:,rand_gen.shape[1]//2, :]\n",
    "    slices[a,:] = slice_1\n",
    "    \n",
    "    # Create a figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    # Plot the image on the axes\n",
    "    ax.imshow(slice_1.T, cmap=\"gray\", origin=\"lower\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.suptitle(\"predicted at time \"+str(a))\n",
    "    plt.savefig(output_path+\"year_\"+str(a)+\"_status_\"+str(status))\n",
    "\n",
    "show_slices(slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d77b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gif from images\n",
    "images = [imageio.imread(output_path+\"year_\"+str(a)+\"_status_\"+str(status)+\".png\") for a in range(11)]\n",
    "print(len(images))\n",
    "\n",
    "# Create a gif\n",
    "gif = imageio.mimsave(output_path+\"status_\"+str(status)+'.gif', images, duration=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
